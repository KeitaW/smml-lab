{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's just a reproduction of https://michaelwornow.net/2024/01/18/counting-params-in-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def count_trainable_params(model, is_human: bool = False):\n",
    "    params: int = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return f\"{params / 1e6:.2f}M\" if is_human else params\n",
    "def count_untrainable_params(model, is_human: bool = False):\n",
    "    params: int = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    return f\"{params / 1e6:.2f}M\" if is_human else params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of trainable parameters: 124.44M\n",
      "Total # of untrainable parameters: 0.00M\n"
     ]
    }
   ],
   "source": [
    "print(\"Total # of trainable parameters:\", count_trainable_params(model, is_human=True))\n",
    "print(\"Total # of untrainable parameters:\", count_untrainable_params(model, is_human=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(wte): Embedding(50257, 768)`\n",
    "\n",
    "Word Token Embedding (WTE), which means: \n",
    "* $V (\\text{total number of tokens in our vocabulary}) = 50257$\n",
    "\n",
    "It is a matrix of size $V(50257)$ by $E(768)$.\n",
    "\n",
    "In other words, our vocabulary has a total of $50257$ unique tokens, and each token is represented by a dense vector of $768$ floating point numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides, $E (\\text{size of the embedding vector}) = 768$\n",
    "\n",
    "$V∗E=50257∗768=38,603,776$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(wpe): Embedding(1024, 768)`\n",
    "\n",
    "Word Position Embedding (WPE), which means:\n",
    "* $P \\text{(the maximum sequence length that our model can handle)} = 1024$\n",
    "It is a matrix of size $P (1024)$ by $E (768)$.\n",
    "\n",
    "This means that the maximum sequence length that our model can handle is $1024$ tokens. This is also referred to as the “context window.”\n",
    "\n",
    "$Params = P∗ E = 1024 ∗ 768 = 786,432$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings from these two layers will get added together to create “position-aware” embeddings of our input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wte | Expected: 38597376\n",
      "wte | True:     38597376\n",
      "wpe | Expected: 786432\n",
      "wpe | True:     786432\n"
     ]
    }
   ],
   "source": [
    "V: int = model.config.vocab_size\n",
    "E: int = model.config.n_embd\n",
    "P: int = model.config.n_positions\n",
    "expected_wte = V * E\n",
    "expected_wpe: int = P * E\n",
    "print(f\"wte | Expected: {expected_wte}\")\n",
    "print(f\"wte | True:     {count_trainable_params(model._modules['wte'])}\")\n",
    "print(f\"wpe | Expected: {expected_wpe}\")\n",
    "print(f\"wpe | True:     {count_trainable_params(model._modules['wpe'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "  (h): ModuleList(\n",
    "    (0-11): 12 x GPT2Block(\n",
    "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "      (attn): GPT2SdpaAttention(\n",
    "        (c_attn): Conv1D(nf=2304, nx=768)\n",
    "        (c_proj): Conv1D(nf=768, nx=768)\n",
    "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
    "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
    "      )\n",
    "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "      (mlp): GPT2MLP(\n",
    "        (c_fc): Conv1D(nf=3072, nx=768)\n",
    "        (c_proj): Conv1D(nf=768, nx=3072)\n",
    "        (act): NewGELUActivation()\n",
    "        (dropout): Dropout(p=0.1, inplace=False)\n",
    "      )\n",
    "    )\n",
    "```\n",
    "Let's breakdown the components of a transformer layer.\n",
    "\n",
    "`ln_1`:\n",
    "\n",
    "* This is a `LayerNorm` layer.\n",
    "* This is responsible for \"normalizing\" the input before it is passed to the attention layer. It normalizes across the last dimension, which is the embedding dimension. This means that the values along the embedding dimension will be normally distributed with mean of 0 and a standard deviation of 1.\n",
    "* The `eps=1e-5 parameter` is the value ϵ added to the denominator. It is used for numerical stability, to prevent division by zero.\n",
    "* The `elementwise_affine=True` parameter means that the layer will learn a bias β and gain γ for each embedding dimension.\n",
    "* The formula for LayerNorm is as follows:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - E[x]}{\\sqrt{Var[x] + \\epsilon}} + \\beta \\\\\n",
    "\\text{where:} \\\\\n",
    "E[x] = \\frac{1}{n}\\sum_{i=1}^n x_i \\quad \\text{(mean)} \\\\\n",
    "Var[x] = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu)^2 \\quad \\text{(variance)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`E[x]` and `Var[x]` are calculated on the fly as the mean of the input (x) across the embedding dimension.\n",
    "Thus, the only learnable parameters here are β and γ, which are vectors of size E (768). \n",
    "\n",
    "Params = 2 * E = 2 * 768 = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln_1 | Expected: 1536\n",
      "ln_1 | True:     1536\n"
     ]
    }
   ],
   "source": [
    "expected_ln_1 = 2 * E\n",
    "print(f\"ln_1 | Expected: {expected_ln_1}\")\n",
    "print(f\"ln_1 | True:     {count_trainable_params(model._modules['h'][0].ln_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPL example: LayerNorm(embedding_dim) =  torch.Size([20, 5, 10])\n",
      "\n",
      "Manual LayerNorm calculation:\n",
      "Input x shape: torch.Size([20, 5, 10])\n",
      "Mean μ shape: torch.Size([20, 5, 1])\n",
      "Variance σ² shape: torch.Size([20, 5, 1])\n",
      "Normalized x̂ shape: torch.Size([20, 5, 10])\n",
      "Output y shape: torch.Size([20, 5, 10])\n",
      "\n",
      "PyTorch LayerNorm output shape: torch.Size([20, 5, 10])\n",
      "Maximum difference: 3.5762786865234375e-07\n",
      "\n",
      "First batch, first position comparison:\n",
      "Manual output[0,0]: tensor([ 0.9932, -0.3416, -0.4775, -1.2895,  0.0837, -2.1132,  0.9516,  0.7095,\n",
      "         0.8504,  0.6334])\n",
      "PyTorch output[0,0]: tensor([ 0.9932, -0.3416, -0.4775, -1.2895,  0.0837, -2.1132,  0.9516,  0.7095,\n",
      "         0.8504,  0.6334], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import LayerNorm\n",
    "# NLP example cf. https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
    "batch, sentence_length, embedding_dim = 20, 5, 10\n",
    "esp = 1e-05\n",
    "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    "layer_norm = LayerNorm(embedding_dim, eps=esp, elementwise_affine=True, bias=True)\n",
    "print(\"NPL example: LayerNorm(embedding_dim) = \", layer_norm(embedding).shape)\n",
    "# Parameters:\n",
    "# - `normalized_shape`: The shape of the input features to be normalized. For embeddings, this is typically (embedding_dim,)\n",
    "# - `eps`: A small value added for numerical stability when normalizing\n",
    "# - `elementwise_affine`: If True, learns an affine transform (gamma, beta) per feature (Default: True)\n",
    "# - `bias`: If True, adds a learnable bias term β after the affine transform. Only relevant if elementwise_affine is True. (Default: True)\n",
    "\n",
    "# How LayerNorm works:\n",
    "# 1. For each sample in the batch:\n",
    "#    - Compute mean μ and variance σ² across the feature dimension\n",
    "#    - Normalize: x̂ = (x - μ) / sqrt(σ² + eps)\n",
    "#    - If elementwise_affine=True: y = γ * x̂ + β\n",
    "\n",
    "# Let's calculate LayerNorm manually to understand how it works under the hood\n",
    "x = embedding  # Shape: (batch, sentence_length, embedding_dim)\n",
    "print(\"\\nManual LayerNorm calculation:\")\n",
    "print(\"Input x shape:\", x.shape)\n",
    "\n",
    "# 1. Calculate mean for each position\n",
    "mean = x.mean(dim=-1, keepdim=True)  # Shape: (batch, sentence_length, 1)\n",
    "print(\"Mean μ shape:\", mean.shape)\n",
    "\n",
    "# 2. Calculate variance for each position\n",
    "var = ((x - mean) ** 2).mean(dim=-1, keepdim=True)  # Shape: (batch, sentence_length, 1)\n",
    "print(\"Variance σ² shape:\", var.shape)\n",
    "\n",
    "# 3. Normalize\n",
    "eps = 1e-05\n",
    "x_norm = (x - mean) / torch.sqrt(var + eps)\n",
    "print(\"Normalized x̂ shape:\", x_norm.shape)\n",
    "\n",
    "# 4. Since weight=1 and bias=0, the output equals x_norm\n",
    "y = x_norm  # In general case: y = weight * x_norm + bias\n",
    "print(\"Output y shape:\", y.shape)\n",
    "\n",
    "# Verify with PyTorch's LayerNorm\n",
    "ln = LayerNorm(embedding_dim, elementwise_affine=True)\n",
    "# Set weight to 1 and bias to 0\n",
    "ln.weight.data.fill_(1.0)\n",
    "ln.bias.data.fill_(0.0)\n",
    "y_torch = ln(x)\n",
    "print(\"\\nPyTorch LayerNorm output shape:\", y_torch.shape)\n",
    "print(\"Maximum difference:\", (y - y_torch).abs().max().item())\n",
    "\n",
    "# Print first batch, first position values for comparison\n",
    "print(\"\\nFirst batch, first position comparison:\")\n",
    "print(\"Manual output[0,0]:\", y[0,0])\n",
    "print(\"PyTorch output[0,0]:\", y_torch[0,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "(attn): GPT2SdpaAttention(\n",
    "    (c_attn): Conv1D(nf=2304, nx=768)\n",
    "    (c_proj): Conv1D(nf=768, nx=768)\n",
    "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
    "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`attn`:\n",
    "* This is a GPT2Attention layer, aka “self-attention”.\n",
    "* This computes the self-attention scores between each token in the input sequence.\n",
    "* It is comprised of four sub-layers:\n",
    "    * `c_attn`:\n",
    "        * This is a Conv1D layer.\n",
    "        * This confused me for a while. What was this Conv1D layer doing in the middle of a transformer layer? I thought it was supposed to be an MLP? My understanding is that it is basically a linear layer, but with the weights transposed. I’m not sure what motivated this design decision, so if anyone knows please leave a comment.\n",
    "        * It is responsible for transforming the input into the query, key, and value matrices for the attention calculation.\n",
    "        * It is a matrix of size E (768) by 3 * E (2304) plus a bias vector of size 3 * E (2304). The 3 * E is because we have 3 inputs to the attention layer: the query, the key, and the value. Each of these inputs is a vector of size E (768), so we have to generate a total of 3 * E (2304) elements.\n",
    "    * `c_proj`:\n",
    "        * This is a Conv1D layer.\n",
    "        * It is responsible for combining the outputs of the attention heads (in our case, there are 12 heads amongst which 768 dims are equally divided, which gives each head a 64-dim output).\n",
    "        * It is a matrix of size E (768) by E (768) plus a bias vector of size E (768).\n",
    "    * `attn_dropout`:\n",
    "        * This is a Dropout layer.  It is responsible for dropping out a fraction (p=0.1) of activations post-attention calculation during training. __This has no trainable parameters__.\n",
    "    * `resid_dropout` is a Dropout layer.  It is responsible for dropping out a fraction (p=0.1) of activations post-projection during training. __This has no trainable parameters.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of $P_{drop} = 0.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html#scaled-dot-product-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_attn shape: torch.Size([768, 2304])\n",
      "c_proj shape: torch.Size([768, 768])\n",
      "c_attn shape: torch.Size([768, 2304])\n",
      "c_proj shape: torch.Size([768, 768])\n"
     ]
    }
   ],
   "source": [
    "attn = model._modules['h'][1].attn\n",
    "print(f\"c_attn shape: {attn.c_attn.weight.shape}\")\n",
    "print(f\"c_proj shape: {attn.c_proj.weight.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Understanding the attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Attention forward function:\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
      "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
      "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n",
      "        if output_attentions or head_mask is not None:\n",
      "            logger.warning_once(\n",
      "                \"`GPT2SdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n",
      "                \"`output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but \"\n",
      "                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n",
      "                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n",
      "            )\n",
      "            return super().forward(\n",
      "                hidden_states=hidden_states,\n",
      "                layer_past=layer_past,\n",
      "                attention_mask=attention_mask,\n",
      "                head_mask=head_mask,\n",
      "                encoder_hidden_states=encoder_hidden_states,\n",
      "                encoder_attention_mask=encoder_attention_mask,\n",
      "                use_cache=use_cache,\n",
      "                output_attentions=output_attentions,\n",
      "            )\n",
      "\n",
      "        bsz, q_len, _ = hidden_states.size()\n",
      "\n",
      "        # Initial attention projections\n",
      "        is_cross_attention = encoder_hidden_states is not None\n",
      "        if is_cross_attention:\n",
      "            if not hasattr(self, \"q_attn\"):\n",
      "                raise ValueError(\n",
      "                    \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n",
      "                    \"Please make sure to instantiate class with `GPT2SdpaAttention(..., is_cross_attention=True)`.\"\n",
      "                )\n",
      "\n",
      "            query = self.q_attn(hidden_states)\n",
      "            key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n",
      "            attention_mask = encoder_attention_mask\n",
      "        else:\n",
      "            query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n",
      "\n",
      "        query = self._split_heads(query, self.num_heads, self.head_dim)\n",
      "        key = self._split_heads(key, self.num_heads, self.head_dim)\n",
      "        value = self._split_heads(value, self.num_heads, self.head_dim)\n",
      "\n",
      "        # Optional kv caching\n",
      "        if layer_past is not None:\n",
      "            past_key = layer_past[0]\n",
      "            past_value = layer_past[1]\n",
      "            key = torch.cat((past_key, key), dim=-2)\n",
      "            value = torch.cat((past_value, value), dim=-2)\n",
      "\n",
      "        present = None\n",
      "        if use_cache is True:\n",
      "            present = (key, value)\n",
      "\n",
      "        # Avoid torch==2.1.2 specific bug for the memory-efficient backend in SDPA\n",
      "        if self.require_contiguous_qkv and query.device.type == \"cuda\" and attention_mask is not None:\n",
      "            query = query.contiguous()\n",
      "            key = key.contiguous()\n",
      "            value = value.contiguous()\n",
      "\n",
      "        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n",
      "        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n",
      "        is_causal = True if attention_mask is None and q_len > 1 and not is_cross_attention else False\n",
      "\n",
      "        attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "            query,\n",
      "            key,\n",
      "            value,\n",
      "            attn_mask=attention_mask,\n",
      "            dropout_p=self.attn_dropout.p if self.training else 0.0,\n",
      "            is_causal=is_causal,\n",
      "        )\n",
      "\n",
      "        # Reshape outputs\n",
      "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "        attn_output = attn_output.view(bsz, q_len, self.embed_dim)\n",
      "\n",
      "        # Final projection\n",
      "        attn_output = self.c_proj(attn_output)\n",
      "        attn_output = self.resid_dropout(attn_output)\n",
      "\n",
      "        return attn_output, present, None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "# Print the actual forward function of the attention layer\n",
    "print(\"GPT2Attention forward function:\")\n",
    "print(inspect.getsource(attn.forward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-attention vs Cross-attention\n",
    "\n",
    "Self-attention and cross-attention are two key variants of the attention mechanism:\n",
    "\n",
    "## Self-attention\n",
    "- Takes a single sequence as input and learns relationships between all positions within that sequence\n",
    "- Query, Key, and Value matrices are all derived from the same input sequence\n",
    "- Used in encoder and decoder self-attention layers\n",
    "- Helps model understand internal relationships and dependencies within a sequence\n",
    "- Example: In \"The cat sat on the mat\", self-attention helps relate \"cat\" to \"sat\" to understand who performed the action\n",
    "\n",
    "## Cross-attention \n",
    "- Takes two different sequences as input and learns relationships between positions across the sequences\n",
    "- Query matrix comes from one sequence, while Key and Value matrices come from another sequence\n",
    "- Used in encoder-decoder attention layers to relate decoder states to encoder outputs\n",
    "- Helps model connect and align information between different sequences\n",
    "- Example: In translation, cross-attention helps align words in source language to corresponding words in target language\n",
    "\n",
    "The key difference is that self-attention operates within a single sequence, while cross-attention operates across two different sequences. GPT-2 uses only self-attention since it's a decoder-only model.\n",
    "So let's extract relevant part from the above forward function.\n",
    "\n",
    "```python\n",
    "# attn forward function (simplified)\n",
    "def forward(\n",
    "    self,\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
    "    layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "    attention_mask: Optional[torch.FloatTensor] = None,\n",
    "    head_mask: Optional[torch.FloatTensor] = None,\n",
    "    encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "    encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "    use_cache: Optional[bool] = False,\n",
    "    output_attentions: Optional[bool] = False,\n",
    ") -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "    # Initial attention projections\n",
    "    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n",
    "\n",
    "    query = self._split_heads(query, self.num_heads, self.head_dim)\n",
    "    key = self._split_heads(key, self.num_heads, self.head_dim)\n",
    "    value = self._split_heads(value, self.num_heads, self.head_dim)\n",
    "    # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n",
    "    # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n",
    "    is_causal = True if attention_mask is None and q_len > 1 and not is_cross_attention else False\n",
    "\n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        attn_mask=attention_mask,\n",
    "        dropout_p=self.attn_dropout.p if self.training else 0.0,\n",
    "        is_causal=is_causal,\n",
    "    )\n",
    "\n",
    "    # Reshape outputs\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    attn_output = attn_output.view(bsz, q_len, self.embed_dim)\n",
    "\n",
    "    # Final projection\n",
    "    attn_output = self.c_proj(attn_output)\n",
    "    attn_output = self.resid_dropout(attn_output)\n",
    "\n",
    "    return attn_output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Efficient implementation equivalent to the following:\n",
    "def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n",
    "        is_causal=False, scale=None, enable_gqa=False) -> torch.Tensor:\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n",
    "    if is_causal:\n",
    "        assert attn_mask is None\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias.to(query.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias = attn_mask + attn_bias\n",
    "\n",
    "    if enable_gqa:\n",
    "        key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\n",
    "        value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\n",
    "\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    return attn_weight @ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
