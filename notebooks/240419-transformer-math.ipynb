{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's just a reproduction of https://michaelwornow.net/2024/01/18/counting-params-in-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def count_trainable_params(model, is_human: bool = False):\n",
    "    params: int = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return f\"{params / 1e6:.2f}M\" if is_human else params\n",
    "def count_untrainable_params(model, is_human: bool = False):\n",
    "    params: int = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    return f\"{params / 1e6:.2f}M\" if is_human else params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of trainable parameters: 124.44M\n",
      "Total # of untrainable parameters: 0.00M\n"
     ]
    }
   ],
   "source": [
    "print(\"Total # of trainable parameters:\", count_trainable_params(model, is_human=True))\n",
    "print(\"Total # of untrainable parameters:\", count_untrainable_params(model, is_human=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(wte): Embedding(50257, 768)`\n",
    "\n",
    "Word Token Embedding (WTE), which means: \n",
    "* $V (\\text{total number of tokens in our vocabulary}) = 50257$\n",
    "\n",
    "It is a matrix of size $V(50257)$ by $E(768)$.\n",
    "\n",
    "In other words, our vocabulary has a total of $50257$ unique tokens, and each token is represented by a dense vector of $768$ floating point numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides, $E (\\text{size of the embedding vector}) = 768$\n",
    "\n",
    "$V∗E=50257∗768=38,603,776$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(wpe): Embedding(1024, 768)`\n",
    "\n",
    "Word Position Embedding (WPE), which means:\n",
    "* $P \\text{(the maximum sequence length that our model can handle)} = 1024$\n",
    "It is a matrix of size $P (1024)$ by $E (768)$.\n",
    "\n",
    "This means that the maximum sequence length that our model can handle is $1024$ tokens. This is also referred to as the “context window.”\n",
    "\n",
    "$Params = P∗ E = 1024 ∗ 768 = 786,432$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings from these two layers will get added together to create “position-aware” embeddings of our input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wte | Expected: 38597376\n",
      "wte | True:     38597376\n",
      "wpe | Expected: 786432\n",
      "wpe | True:     786432\n"
     ]
    }
   ],
   "source": [
    "V: int = model.config.vocab_size\n",
    "E: int = model.config.n_embd\n",
    "P: int = model.config.n_positions\n",
    "expected_wte = V * E\n",
    "expected_wpe: int = P * E\n",
    "print(f\"wte | Expected: {expected_wte}\")\n",
    "print(f\"wte | True:     {count_trainable_params(model._modules['wte'])}\")\n",
    "print(f\"wpe | Expected: {expected_wpe}\")\n",
    "print(f\"wpe | True:     {count_trainable_params(model._modules['wpe'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "  (h): ModuleList(\n",
    "    (0-11): 12 x GPT2Block(\n",
    "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "      (attn): GPT2SdpaAttention(\n",
    "        (c_attn): Conv1D(nf=2304, nx=768)\n",
    "        (c_proj): Conv1D(nf=768, nx=768)\n",
    "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
    "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
    "      )\n",
    "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "      (mlp): GPT2MLP(\n",
    "        (c_fc): Conv1D(nf=3072, nx=768)\n",
    "        (c_proj): Conv1D(nf=768, nx=3072)\n",
    "        (act): NewGELUActivation()\n",
    "        (dropout): Dropout(p=0.1, inplace=False)\n",
    "      )\n",
    "    )\n",
    "```\n",
    "Let's breakdown the components of a transformer layer.\n",
    "\n",
    "`ln_1`:\n",
    "\n",
    "* This is a `LayerNorm` layer.\n",
    "* This is responsible for \"normalizing\" the input before it is passed to the attention layer. It normalizes across the last dimension, which is the embedding dimension. This means that the values along the embedding dimension will be normally distributed with mean of 0 and a standard deviation of 1.\n",
    "* The `eps=1e-5 parameter` is the value ϵ added to the denominator. It is used for numerical stability, to prevent division by zero.\n",
    "* The `elementwise_affine=True` parameter means that the layer will learn a bias β and gain γ for each embedding dimension.\n",
    "* The formula for LayerNorm is as follows:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - E[x]}{\\sqrt{Var[x] + \\epsilon}} + \\beta \\\\\n",
    "\\text{where:} \\\\\n",
    "E[x] = \\frac{1}{n}\\sum_{i=1}^n x_i \\quad \\text{(mean)} \\\\\n",
    "Var[x] = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu)^2 \\quad \\text{(variance)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`E[x]` and `Var[x]` are calculated on the fly as the mean of the input (x) across the embedding dimension.\n",
    "Thus, the only learnable parameters here are β and γ, which are vectors of size E (768). \n",
    "\n",
    "Params = 2 * E = 2 * 768 = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln_1 | Expected: 1536\n",
      "ln_1 | True:     1536\n"
     ]
    }
   ],
   "source": [
    "expected_ln_1 = 2 * E\n",
    "print(f\"ln_1 | Expected: {expected_ln_1}\")\n",
    "print(f\"ln_1 | True:     {count_trainable_params(model._modules['h'][0].ln_1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPL example: LayerNorm(embedding_dim) =  torch.Size([20, 5, 10])\n",
      "\n",
      "Manual LayerNorm calculation:\n",
      "Input x shape: torch.Size([20, 5, 10])\n",
      "Mean μ shape: torch.Size([20, 5, 1])\n",
      "Variance σ² shape: torch.Size([20, 5, 1])\n",
      "Normalized x̂ shape: torch.Size([20, 5, 10])\n",
      "Output y shape: torch.Size([20, 5, 10])\n",
      "\n",
      "PyTorch LayerNorm output shape: torch.Size([20, 5, 10])\n",
      "Maximum difference: 4.76837158203125e-07\n",
      "\n",
      "First batch, first position comparison:\n",
      "Manual output[0,0]: tensor([-0.1727,  0.7934, -1.1282, -1.1294, -1.0008,  1.0818,  1.3280,  0.4187,\n",
      "         1.0586, -1.2493])\n",
      "PyTorch output[0,0]: tensor([-0.1727,  0.7934, -1.1282, -1.1294, -1.0008,  1.0818,  1.3280,  0.4187,\n",
      "         1.0586, -1.2493], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import LayerNorm\n",
    "# NLP example cf. https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
    "batch, sentence_length, embedding_dim = 20, 5, 10\n",
    "esp = 1e-05\n",
    "embedding = torch.randn(batch, sentence_length, embedding_dim)\n",
    "layer_norm = LayerNorm(embedding_dim, eps=esp, elementwise_affine=True, bias=True)\n",
    "print(\"NPL example: LayerNorm(embedding_dim) = \", layer_norm(embedding).shape)\n",
    "# Parameters:\n",
    "# - `normalized_shape`: The shape of the input features to be normalized. For embeddings, this is typically (embedding_dim,)\n",
    "# - `eps`: A small value added for numerical stability when normalizing\n",
    "# - `elementwise_affine`: If True, learns an affine transform (gamma, beta) per feature (Default: True)\n",
    "# - `bias`: If True, adds a learnable bias term β after the affine transform. Only relevant if elementwise_affine is True. (Default: True)\n",
    "\n",
    "# How LayerNorm works:\n",
    "# 1. For each sample in the batch:\n",
    "#    - Compute mean μ and variance σ² across the feature dimension\n",
    "#    - Normalize: x̂ = (x - μ) / sqrt(σ² + eps)\n",
    "#    - If elementwise_affine=True: y = γ * x̂ + β\n",
    "\n",
    "# Let's calculate LayerNorm manually to understand how it works under the hood\n",
    "x = embedding  # Shape: (batch, sentence_length, embedding_dim)\n",
    "print(\"\\nManual LayerNorm calculation:\")\n",
    "print(\"Input x shape:\", x.shape)\n",
    "\n",
    "# 1. Calculate mean for each position\n",
    "mean = x.mean(dim=-1, keepdim=True)  # Shape: (batch, sentence_length, 1)\n",
    "print(\"Mean μ shape:\", mean.shape)\n",
    "\n",
    "# 2. Calculate variance for each position\n",
    "var = ((x - mean) ** 2).mean(dim=-1, keepdim=True)  # Shape: (batch, sentence_length, 1)\n",
    "print(\"Variance σ² shape:\", var.shape)\n",
    "\n",
    "# 3. Normalize\n",
    "eps = 1e-05\n",
    "x_norm = (x - mean) / torch.sqrt(var + eps)\n",
    "print(\"Normalized x̂ shape:\", x_norm.shape)\n",
    "\n",
    "# 4. Since weight=1 and bias=0, the output equals x_norm\n",
    "y = x_norm  # In general case: y = weight * x_norm + bias\n",
    "print(\"Output y shape:\", y.shape)\n",
    "\n",
    "# Verify with PyTorch's LayerNorm\n",
    "ln = LayerNorm(embedding_dim, elementwise_affine=True)\n",
    "# Set weight to 1 and bias to 0\n",
    "ln.weight.data.fill_(1.0)\n",
    "ln.bias.data.fill_(0.0)\n",
    "y_torch = ln(x)\n",
    "print(\"\\nPyTorch LayerNorm output shape:\", y_torch.shape)\n",
    "print(\"Maximum difference:\", (y - y_torch).abs().max().item())\n",
    "\n",
    "# Print first batch, first position values for comparison\n",
    "print(\"\\nFirst batch, first position comparison:\")\n",
    "print(\"Manual output[0,0]:\", y[0,0])\n",
    "print(\"PyTorch output[0,0]:\", y_torch[0,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "(attn): GPT2SdpaAttention(\n",
    "    (c_attn): Conv1D(nf=2304, nx=768)\n",
    "    (c_proj): Conv1D(nf=768, nx=768)\n",
    "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
    "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`attn`:\n",
    "* This is a GPT2Attention layer, aka “self-attention”.\n",
    "* This computes the self-attention scores between each token in the input sequence.\n",
    "* It is comprised of four sub-layers:\n",
    "    * `c_attn`:\n",
    "        * This is a Conv1D layer.\n",
    "        * This confused me for a while. What was this Conv1D layer doing in the middle of a transformer layer? I thought it was supposed to be an MLP? My understanding is that it is basically a linear layer, but with the weights transposed. I’m not sure what motivated this design decision, so if anyone knows please leave a comment.\n",
    "        * It is responsible for transforming the input into the query, key, and value matrices for the attention calculation.\n",
    "        * It is a matrix of size E (768) by 3 * E (2304) plus a bias vector of size 3 * E (2304). The 3 * E is because we have 3 inputs to the attention layer: the query, the key, and the value. Each of these inputs is a vector of size E (768), so we have to generate a total of 3 * E (2304) elements.\n",
    "    * `c_proj`:\n",
    "        * This is a Conv1D layer.\n",
    "        * It is responsible for combining the outputs of the attention heads (in our case, there are 12 heads amongst which 768 dims are equally divided, which gives each head a 64-dim output).\n",
    "        * It is a matrix of size E (768) by E (768) plus a bias vector of size E (768).\n",
    "    * `attn_dropout`:\n",
    "        * This is a Dropout layer.  It is responsible for dropping out a fraction (p=0.1) of activations post-attention calculation during training. __This has no trainable parameters__.\n",
    "    * `resid_dropout` is a Dropout layer.  It is responsible for dropping out a fraction (p=0.1) of activations post-projection during training. __This has no trainable parameters.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Residual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
